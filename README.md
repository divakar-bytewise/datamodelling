# Data Modelling Project

## Overview
This repository demonstrates **data modeling concepts** using PySpark and Databricks with Unity Catalog.  
It covers schema design, staging layers, surrogate vs natural keys, clustering strategies, and Delta Lake optimizations.

The goal of this project is to provide hands-on examples for:
- Defining custom schemas in PySpark
- Creating Unity Catalog tables
- Using **surrogate keys** and **natural keys**
- Understanding CDC (Change Data Capture) in Delta tables

